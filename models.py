from keras.layers import Conv2D, MaxPool2D
from keras.layers import Embedding, Concatenate, BatchNormalization
from keras.layers.core import *
from keras.layers.recurrent import LSTM
from keras.models import *
from keras.optimizers import Adam, SGD, RMSprop


def model_custom(embeddingMatrix):
    x1 = Input(shape=(100,), name='main_input1')
    x2 = Input(shape=(100,), name='main_input2')
    x3 = Input(shape=(100,), name='main_input3')
    embeddingLayer = Embedding(input_dim=embeddingMatrix.shape[0], output_dim=300, weights=[embeddingMatrix],
                               mask_zero=True, input_length=100, trainable=True)
    emb1 = embeddingLayer(x1)
    emb2 = embeddingLayer(x2)
    emb3 = embeddingLayer(x3)
    emb1 = Activation('tanh')(emb1)
    emb2 = Activation('tanh')(emb2)
    emb3 = Activation('tanh')(emb3)
    expand_dim = Lambda(lambda x: K.expand_dims(x, axis=-1))
    emb1 = expand_dim(emb1)
    emb2 = expand_dim(emb2)
    emb3 = expand_dim(emb3)
    flatten = Flatten()
    conv_0 = Conv2D(128, kernel_size=(2, 300), padding='valid', activation='relu')
    conv_1 = Conv2D(128, kernel_size=(3, 300), padding='valid', activation='relu')
    conv_2 = Conv2D(128, kernel_size=(4, 300), padding='valid', activation='relu')
    maxpool_0 = MaxPool2D(pool_size=(99, 1), strides=(1, 1), padding='valid')
    maxpool_1 = MaxPool2D(pool_size=(98, 1), strides=(1, 1), padding='valid')
    maxpool_2 = MaxPool2D(pool_size=(97, 1), strides=(1, 1), padding='valid')
    dense_func = Dense(600, activation='relu', name="dense")
    dense_final = Dense(units=4, activation='softmax')
    # reshape_func = Reshape((sentence_length, embedding_dim, 1))
    batch_norm1 = BatchNormalization()
    batch_norm2 = BatchNormalization()
    batch_norm3 = BatchNormalization()
    dropout = Dropout(0.2)
    activation = Activation('relu')
    conv_0_e1 = conv_0(emb1)
    conv_0_e2 = conv_0(emb2)
    conv_0_e3 = conv_0(emb3)
    conv_1_e1 = conv_1(emb1)
    conv_1_e2 = conv_1(emb2)
    conv_1_e3 = conv_1(emb3)
    conv_2_e1 = conv_2(emb1)
    conv_2_e2 = conv_2(emb2)
    conv_2_e3 = conv_2(emb3)
    maxpool_0_e1 = maxpool_0(conv_0_e1)
    maxpool_0_e2 = maxpool_0(conv_0_e2)
    maxpool_0_e3 = maxpool_0(conv_0_e3)
    maxpool_1_e1 = maxpool_1(conv_1_e1)
    maxpool_1_e2 = maxpool_1(conv_1_e2)
    maxpool_1_e3 = maxpool_1(conv_1_e3)
    maxpool_2_e1 = maxpool_2(conv_2_e1)
    maxpool_2_e2 = maxpool_2(conv_2_e2)
    maxpool_2_e3 = maxpool_2(conv_2_e3)
    out_0_e1 = flatten(maxpool_0_e1)
    out_0_e2 = flatten(maxpool_0_e2)
    out_0_e3 = flatten(maxpool_0_e3)
    out_1_e1 = flatten(maxpool_1_e1)
    out_1_e2 = flatten(maxpool_1_e2)
    out_1_e3 = flatten(maxpool_1_e3)
    out_2_e1 = flatten(maxpool_2_e1)
    out_2_e2 = flatten(maxpool_2_e2)
    out_2_e3 = flatten(maxpool_2_e3)
    e1 = Concatenate(axis=-1)([out_0_e1, out_1_e1, out_2_e1])
    e2 = Concatenate(axis=-1)([out_0_e2, out_1_e2, out_2_e2])
    e3 = Concatenate(axis=-1)([out_0_e3, out_1_e3, out_2_e3])
    expand_dim = Lambda(lambda x: K.expand_dims(x, axis=-2))
    e1 = activation(e1)
    e2 = activation(e2)
    e3 = activation(e3)
    # e1 = batch_norm1(e1)
    # e2 = batch_norm2(e2)
    # e3 = batch_norm3(e3)
    e1 = expand_dim(e1)
    e2 = expand_dim(e2)
    e3 = expand_dim(e3)
    e = Concatenate(axis=-2)([e1, e2, e3])
    # e = Reshape((3, 384))(e)
    lstm_1 = LSTM(300, return_sequences=True)(e)
    first_timestep = Lambda(lambda x: x[:, 0, :])(lstm_1)
    last_timestep = Lambda(lambda x: x[:, -1, :])(lstm_1)

    att = AttentionWeightedAverage()(lstm_1)
    att = Dense(600, activation='relu')(att)
    out = Dense(4, activation='softmax')(att)
    model = Model([x1, x2, x3], out)
    print(model.summary())
    return model


model = model_custom()
